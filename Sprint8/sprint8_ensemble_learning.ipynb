{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint8 アンサンブル学習\n",
    "---\n",
    "3種類のアンサンブル学習をスクラッチ実装していきます。そして、それぞれの効果を小さめのデータセットで確認します。\n",
    "\n",
    "-    ブレンディング\n",
    "-    バギング\n",
    "-    スタッキング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小さなデータセットの用意\n",
    "\n",
    "以前も利用した回帰のデータセットを用意します。\n",
    "\n",
    "House Prices: Advanced Regression Techniques\n",
    "\n",
    "この中のtrain.csvをダウンロードし、目的変数としてSalePrice、説明変数として、GrLivAreaとYearBuiltを使います。\n",
    "\n",
    "train.csvを学習データ8割、検証データ2割に分割してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168, 2) (292, 2) (1460, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "X = df[['GrLivArea', 'YearBuilt']].values\n",
    "y = df['SalePrice'].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 訓練データと検証データの分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(X_train.shape, X_test.shape, X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】ブレンディングのスクラッチ実装\n",
    "---\n",
    "ブレンディング をスクラッチ実装し、単一モデルより精度があがる例を 最低3つ 示してください。精度があがるとは、検証データに対する平均二乗誤差（MSE）が小さくなることを指します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単一モデルにおいて前処理、ハイパーパラメータなどの設定をしない場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr mse :  32711.07748550501\n",
      "svm mse :  55442.18585049607\n",
      "cart mse :  38523.850456621\n",
      "knn mse :  34761.789041095886\n",
      "blending mse :  30612.67605827354\n",
      "ave mse :  33286.82526320544\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 線形回帰\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_y_pred = lr.predict(X_test)\n",
    "print(\"lr mse : \", mean_absolute_error(y_test, lr_y_pred))\n",
    "\n",
    "# SVM\n",
    "svm = SVR()\n",
    "svm.fit(X_train, y_train)\n",
    "svm_y_pred = svm.predict(X_test)\n",
    "print(\"svm mse : \", mean_absolute_error(y_test, svm_y_pred))\n",
    "\n",
    "# 決定木\n",
    "cart = DecisionTreeRegressor()\n",
    "cart.fit(X_train, y_train)\n",
    "cart_y_pred = cart.predict(X_test)\n",
    "print(\"cart mse : \", mean_absolute_error(y_test, cart_y_pred))\n",
    "\n",
    "# Kmeans\n",
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train, y_train)\n",
    "knn_y_pred = knn.predict(X_test)\n",
    "print(\"knn mse : \", mean_absolute_error(y_test, knn_y_pred))\n",
    "\n",
    "# 各モデルに対して重み付けをして、足し合わせる。\n",
    "blending_y_pred = 0.5 * lr_y_pred + \\\n",
    "                  0.1 * svm_y_pred + \\\n",
    "                  0.2 * cart_y_pred + \\\n",
    "                  0.2 * knn_y_pred\n",
    "            \n",
    "print(\"blending mse : \", mean_absolute_error(y_test, blending_y_pred))\n",
    "\n",
    "ave_y_pred = (lr_y_pred + svm_y_pred + cart_y_pred + cart_y_pred)/4\n",
    "print(\"ave mse : \", mean_absolute_error(y_test, ave_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単一モデルにおいて前処理、ハイパーパラメータなどの設定を行った場合→多様モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr mse :  32711.07748550502\n",
      "svm mse :  31511.233790687565\n",
      "cart mse :  27769.37648917789\n",
      "knn mse :  26977.90525114155\n",
      "blending mse :  27667.65696696641\n",
      "ave mse :  28517.983694043774\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) #trainデータのみFitを実行する\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# 線形回帰\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_std, y_train)\n",
    "lr_y_pred = lr.predict(X_test_std)\n",
    "print(\"lr mse : \", mean_absolute_error(y_test, lr_y_pred))\n",
    "\n",
    "# SVM\n",
    "svm = SVR(kernel='linear', C=10 )\n",
    "svm.fit(X_train, y_train)\n",
    "svm_y_pred = svm.predict(X_test)\n",
    "print(\"svm mse : \", mean_absolute_error(y_test, svm_y_pred))\n",
    "\n",
    "# 決定木\n",
    "cart = DecisionTreeRegressor(random_state=0, max_depth=7)\n",
    "cart.fit(X_train, y_train)\n",
    "cart_y_pred = cart.predict(X_test)\n",
    "print(\"cart mse : \", mean_absolute_error(y_test, cart_y_pred))\n",
    "\n",
    "# Kmeans\n",
    "knn = KNeighborsRegressor(n_neighbors=9)\n",
    "knn.fit(X_train_std, y_train)\n",
    "knn_y_pred = knn.predict(X_test_std)\n",
    "print(\"knn mse : \", mean_absolute_error(y_test, knn_y_pred))\n",
    "\n",
    "# 各モデルに対して重み付けをして、足し合わせる。\n",
    "blending_y_pred = 0.05 * lr_y_pred + \\\n",
    "                  0.05 * svm_y_pred + \\\n",
    "                  0.3 * cart_y_pred + \\\n",
    "                  0.6 * cart_y_pred\n",
    "            \n",
    "print(\"blending mse : \", mean_absolute_error(y_test, blending_y_pred))\n",
    "\n",
    "ave_y_pred = (lr_y_pred + svm_y_pred + cart_y_pred + cart_y_pred)/4\n",
    "print(\"ave mse : \", mean_absolute_error(y_test, ave_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 考察\n",
    "---\n",
    "単一モデルよりもブレンディングしたことによってMSEの精度が高くなった。\n",
    "適用した手法：線形回帰、SVM、決定木、Kmeansの４つの手法\n",
    "前処理としては、データを標準化にし、線形回帰とKmeansモデルへ適用した。SVMと決定木は逆にMSEの値が悪くなったので、標準化しないデータで学習させた。\n",
    "各モデルのハイパーパラメータをチューニングすることによって精度が向上したと言える。\n",
    "特に、決定木とKmeansの２つに関してはかなり良い結果となった。\n",
    "\n",
    "最後に各モデルの結果を見て、重み付けをすることによって、更に精度が高くなった。\n",
    "良いモデルに関しては比較的重み付けの比重を多くした。\n",
    "\n",
    "また、各モデルの予測値を平均化した場合でも、そこそこ精度が良かった。\n",
    "ただ、ハイパーパラメータの設定によっては一部の単一モデルより精度が下がっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】バギングのスクラッチ実装\n",
    "---\n",
    "バギング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。\n",
    "\n",
    "バギングとは\n",
    "\n",
    "バギングは入力データの選び方を多様化する方法です。訓練データから重複を許した上でランダムに抜き出すことで、N種類のサブセット（ ブートストラップサンプル ）を作り出します。それらによってモデルをN個学習し、推定結果の平均をとります。ブレンディングと異なり、それぞれの重み付けを変えることはありません。\n",
    "\n",
    "sklearn.model_selection.train_test_split — scikit-learn 0.21.3 documentation\n",
    "\n",
    "推定結果の平均をとる部分はブレンディングと同様の実装になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr mse : 32141.995843820063\n",
      "svm mse : 28304.659350887352\n",
      "tree mse : 26090.310849895643\n",
      "knn mse : 26864.505050505057\n",
      "Average mse :  15414.124428038422\n",
      "Sample1, mse : 50874.54448187247\n",
      "Sample2, mse : 52567.54183042638\n",
      "Sample3, mse : 49423.76511073222\n",
      "Sample4, mse : 45714.730972450074\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "models = [\n",
    "            ['lr', LinearRegression()],\n",
    "            ['svm', SVR(kernel='linear', C=10 )],\n",
    "            ['tree', DecisionTreeRegressor(random_state=0, max_depth=7)],\n",
    "            ['knn', KNeighborsRegressor(n_neighbors=9)]\n",
    "        ]\n",
    "\n",
    "y_pred_list = []\n",
    "y_test_list = []\n",
    "for model_name, model in models:\n",
    "    \n",
    "    # 全体の6割をサンプリングする\n",
    "    data = df.sample(frac=0.6, replace=True)\n",
    "\n",
    "    X = data[['GrLivArea', 'YearBuilt']].values\n",
    "    y = data['SalePrice'].values\n",
    "\n",
    "    # 訓練データと検証データの分割\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_list.append(y_pred)\n",
    "    y_test_list.append(y_test)\n",
    "    print(\"{} mse : {}\".format(model_name, mean_absolute_error(y_test, y_pred)))\n",
    "\n",
    "ave_y_test = np.array(y_test_list).mean(axis=0)\n",
    "ave_y_pred = np.array(y_pred_list).mean(axis=0)\n",
    "print(\"Average mse : \", mean_absolute_error(ave_y_test, ave_y_pred))\n",
    "\n",
    "for i, test in enumerate(y_test_list):\n",
    "    print(\"Sample{}, mse : {}\".format(i+1, mean_absolute_error(test, ave_y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 考察\n",
    "---\n",
    "各モデルで使用したy_testデータでMSEを算出した結果、単一モデルの方が良い結果となった。\n",
    "これは想定外の結果ですが、何かバギングのやり方が間違っているのでしょうか？\n",
    "\n",
    "また、各モデルで使用したy_testの平均値のMSEの値は良いが、そもそもy_testで使用するデータを平均するのはおかしいと思うのですが、どうなのでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】スタッキングのスクラッチ実装\n",
    "---\n",
    "スタッキング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "X = df[['GrLivArea', 'YearBuilt']].values\n",
    "y = df['SalePrice'].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 訓練データと検証データの分割\n",
    "X_train_org, X_test, y_train_org, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr mse: 32710.256655269444\n",
      "svm mse: 55464.79206621753\n",
      "cart mse: 30528.340410958906\n",
      "knn mse: 33322.31534246575\n",
      "blending mse :  30111.604666393647\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "models = [\n",
    "            ['lr', LinearRegression()],\n",
    "            ['svm', SVR()],\n",
    "            ['cart', DecisionTreeRegressor()],\n",
    "            ['knn', KNeighborsRegressor()]\n",
    "        ]\n",
    "\n",
    "\n",
    "# クロスバリデーション\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "blend_train = np.zeros((X_train_org.shape[0], len(models)))\n",
    "blend_test = np.zeros((X_test.shape[0], len(models)))\n",
    "\n",
    "for j, (name, model) in enumerate(models):\n",
    "    \n",
    "    blend_test_j = np.zeros((X_test.shape[0], n_folds))\n",
    "\n",
    "    # データセットを分割\n",
    "    for i, (train_idx, valid_idx) in enumerate(kf.split(X_train_org, y_train_org)):\n",
    "        \n",
    "        X_train, y_train = X_train_org[train_idx], y_train_org[train_idx]\n",
    "        X_valid, y_valid = X_train_org[valid_idx], y_train_org[valid_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        blend_train[valid_idx, j] = model.predict(X_valid)\n",
    "        blend_test_j[:, i] = model.predict(X_test)\n",
    "\n",
    "    # クロスバリデーションした予測値を平均化\n",
    "    blend_test[:, j] = blend_test_j.mean(axis=1)\n",
    "    print(\"{} mse: {}\".format(name, mean_absolute_error(y_test, blend_test_j.mean(axis=1))))\n",
    "\n",
    "\n",
    "blender = LinearRegression()\n",
    "blender.fit(blend_train, y_train_org)\n",
    "blender_pred = blender.predict(blend_test)\n",
    "\n",
    "print(\"blending mse : \", mean_absolute_error(y_test, blender_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 考察\n",
    "---\n",
    "スタッキングで実行した結果、MSEの精度は上がった。\n",
    "\n",
    "ただ、各モデルのハイパーパラメータを設定すると、以下のように一部の単一モデル（cart）より精度が落ちる場合がある。\n",
    "これは、起こり得ることなのでしょうか？\n",
    "\n",
    "lr mse: 32710.256655269444  \n",
    "svm mse: 31514.52887818637   \n",
    "cart mse: 25469.247447367132  \n",
    "knn mse: 32499.18325722983   \n",
    "blending mse :  28163.498199440211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr mse: 32710.256655269444\n",
      "svm mse: 31514.52887818637\n",
      "cart mse: 25469.247447367132\n",
      "knn mse: 32499.18325722983\n",
      "blending mse :  28163.49819944021\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "models = [\n",
    "            ['lr', LinearRegression()],\n",
    "            ['svm', SVR(kernel='linear', C=10 )],\n",
    "            ['cart', DecisionTreeRegressor(random_state=0, max_depth=7)],\n",
    "            ['knn', KNeighborsRegressor(n_neighbors=9)]\n",
    "        ]\n",
    "\n",
    "\n",
    "# クロスバリデーション\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "blend_train = np.zeros((X_train_org.shape[0], len(models)))\n",
    "blend_test = np.zeros((X_test.shape[0], len(models)))\n",
    "\n",
    "for j, (name, model) in enumerate(models):\n",
    "    \n",
    "    blend_test_j = np.zeros((X_test.shape[0], n_folds))\n",
    "\n",
    "    # データセットを分割\n",
    "    for i, (train_idx, valid_idx) in enumerate(kf.split(X_train_org, y_train_org)):\n",
    "        \n",
    "        X_train, y_train = X_train_org[train_idx], y_train_org[train_idx]\n",
    "        X_valid, y_valid = X_train_org[valid_idx], y_train_org[valid_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        blend_train[valid_idx, j] = model.predict(X_valid)\n",
    "        blend_test_j[:, i] = model.predict(X_test)\n",
    "\n",
    "    # クロスバリデーションした予測値を平均化\n",
    "    blend_test[:, j] = blend_test_j.mean(axis=1)\n",
    "    print(\"{} mse: {}\".format(name, mean_absolute_error(y_test, blend_test_j.mean(axis=1))))\n",
    "\n",
    "\n",
    "blender = LinearRegression()\n",
    "blender.fit(blend_train, y_train_org)\n",
    "blender_pred = blender.predict(blend_test)\n",
    "\n",
    "print(\"blending mse : \", mean_absolute_error(y_test, blender_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
