{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8de54133-eb3f-4a8d-b114-710023745a37",
   "metadata": {},
   "source": [
    "# 1次元の畳み込みニューラルネットワークスクラッチ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd4d682-e509-483e-b9fb-772609ac2cd1",
   "metadata": {},
   "source": [
    "## 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成\n",
    "---\n",
    "チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。基本構造は前のSprintで作成した全結合層のFCクラスと同じになります。なお、重みの初期化に関するクラスは必要に応じて作り変えてください。Xavierの初期値などを使う点は全結合層と同様です。\n",
    "\n",
    "ここでは*パディング*は考えず*ストライド*も1に固定します。また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。この部分の拡張はアドバンス課題とします。\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。\n",
    "\n",
    "$$\n",
    "a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_{s+b}\n",
    "$$\n",
    "\n",
    "$a_i$: 出力される配列のi番目の値\n",
    "\n",
    "$F$: フィルタのサイズ\n",
    "\n",
    "$x_{(i+s)}$: 入力の配列の(i+s)番目の値\n",
    "\n",
    "$w_{s}$: 重みの配列のs番目の値\n",
    "\n",
    "$b$: バイアス項\n",
    "\n",
    "すべてスカラーです。\n",
    "\n",
    "次に更新式です。ここがAdaGradなどに置き換えられる点は全結合層と同様です。\n",
    "$$\n",
    "w_s^{\\prime} = w_s - \\alpha \\frac{\\partial L}{\\partial w_s} \\\\\n",
    "b^{\\prime} = b - \\alpha \\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "α: 学習率\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a_i}$: ws に関する損失$L$の勾配\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b}$: b に関する損失$L$の勾配\n",
    "\n",
    "勾配$\\frac{\\partial L}{\\partial w_s}$や$\\frac{\\partial L}{\\partial b}$を求めるためのバックプロパゲーションの数式が以下です。\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_s} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}x_{(i+s)}\\\\\n",
    "\\frac{\\partial L}{\\partial b} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a_i}$: 勾配の配列のi番目の値\n",
    "\n",
    "$N_{out}$: 出力のサイズ\n",
    "\n",
    "前の層に流す誤差の数式は以下です。\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x_j} = \\sum_{s=0}^{F-1} \\frac{\\partial L}{\\partial a_{(j-s)}}w_s$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_j}$: 前の層に流す誤差の配列のj番目の値\n",
    "\n",
    "ただし、$j−s<0$または$j−s>N_{out}−1$のとき$\\frac{\\partial L}{\\partial a_{(j-s)}} =0$です。\n",
    "\n",
    "全結合層との大きな違いは、重みが複数の特徴量に対して共有されていることです。この場合は共有されている分の誤差をすべて足すことで勾配を求めます。計算グラフ上での分岐はバックプロパゲーションの際に誤差の足し算をすれば良いことになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc4a835f-848f-4f67-9ab4-68b99167b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleConv1d:\n",
    "    \"\"\"\n",
    "    畳み込み層\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : ndarray\n",
    "      重み\n",
    "    B : ndarray\n",
    "      バイアス\n",
    "    \"\"\"\n",
    "    def __init__(self, W, B):\n",
    "        self.stride = 1     #今回は1固定\n",
    "        self.padding = 0    #今回は無し\n",
    "        self.batch_size = 1 # #今回は1固定\n",
    "\n",
    "        self.W = W\n",
    "        self.B = B\n",
    "\n",
    "        # 微分した重みとバイアス\n",
    "        self.dW = None\n",
    "        self.dB = None\n",
    "\n",
    "        self.x = None\n",
    "        self.x_matrix = None\n",
    "        self.dZ = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "\n",
    "        self.x = X  # backwardで使うため保持\n",
    "\n",
    "        idx = 0\n",
    "        # n_row = len(X) - len(self.W) + self.stride\n",
    "        n_output = self._get_output_size()\n",
    "        x_list = []\n",
    "        for i in range(n_output):\n",
    "            # print(X[idx:idx + len(self.W)])\n",
    "            x_list.append(X[idx:idx + len(self.W)])\n",
    "            idx += self.stride\n",
    "\n",
    "        self.x_matrix = np.array(x_list)\n",
    "\n",
    "        A = np.sum(self.W.transpose() * np.array(x_list), axis=1) + self.B\n",
    "\n",
    "        return A\n",
    "\n",
    "    def _backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "\n",
    "        # n_row = len(self.x) - len(self.W) + self.stride\n",
    "        n_output = self._get_output_size()\n",
    "        idx = diff = len(self.x) - len(self.W)\n",
    "        w_list = []\n",
    "        for i in range(n_output):\n",
    "            _w = np.pad(self.W, [idx-diff, diff],'constant')\n",
    "            w_list.append(_w)\n",
    "            diff -= self.stride\n",
    "\n",
    "        dx = np.sum(dA.reshape(-1,1) * np.array(w_list), axis=0)\n",
    "        self.dW = np.sum(self.x_matrix.T * dA, axis=1)\n",
    "        self.dB = np.array(dA.sum())\n",
    "\n",
    "        return dx\n",
    "    \n",
    "    #問題2\n",
    "    def _get_output_size(self):\n",
    "        '''\n",
    "        出力サイズの計算 n_output = (n_input + 2*padding - n_filters)/stride + 1\n",
    "        '''\n",
    "\n",
    "        return int((len(self.x) + 2*self.padding - len(self.W))/ self.stride + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17118c4-3031-46a2-acb0-d4540eda4279",
   "metadata": {},
   "source": [
    "## 【問題2】1次元畳み込み後の出力サイズの計算\n",
    "---\n",
    "畳み込みを行うと特徴量の数が変化します。どのように変化するかは以下の数式から求められます。パディングやストライドも含めています。この計算を行う関数を作成してください。\n",
    "$$\n",
    "N_{out} =  \\frac{N_{in}+2P-F}{S} + 1\n",
    "$$\n",
    "$N_{out}$: 出力のサイズ（特徴量の数）\n",
    "\n",
    "$N_{in}$: 入力のサイズ（特徴量の数）\n",
    "\n",
    "$P$: ある方向へのパディングの数\n",
    "\n",
    "$F$: フィルタのサイズ\n",
    "\n",
    "$S$: ストライドのサイズ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b6a0c3-476b-42ff-bb89-831b24ee274d",
   "metadata": {},
   "source": [
    "## 【問題3】小さな配列での1次元畳み込み層の実験\n",
    "---\n",
    "次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。\n",
    "\n",
    "入力x、重みw、バイアスbを次のようにします。\n",
    "\n",
    "```python\n",
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])    \n",
    "```\n",
    "\n",
    "フォワードプロパゲーションをすると出力は次のようになります。\n",
    "```python\n",
    "a = np.array([35, 50])\n",
    "```\n",
    "次にバックプロパゲーションを考えます。誤差は次のようであったとします。\n",
    "```python\n",
    "delta_a = np.array([10, 20])\n",
    "```\n",
    "バックプロパゲーションをすると次のような値になります。\n",
    "```python\n",
    "delta_b = np.array([30])\n",
    "delta_w = np.array([50, 80, 110])\n",
    "delta_x = np.array([30, 110, 170, 140])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c446d09-2ce0-4c6d-9b08-9632965ec3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a :  [35 50]\n",
      "dB :  30\n",
      "dW :  [ 50  80 110]\n",
      "dx :  [ 30 110 170 140]\n"
     ]
    }
   ],
   "source": [
    " x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "\n",
    "conv1d = SimpleConv1d(w, b)\n",
    "a = conv1d._forward(x)\n",
    "print(\"a : \", a)\n",
    "\n",
    "delta_a = np.array([10, 20])\n",
    "\n",
    "dx = conv1d._backward(delta_a)\n",
    "print(\"dB : \", conv1d.dB)\n",
    "print(\"dW : \", conv1d.dW)\n",
    "print(\"dx : \", dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0941bc69-2124-42be-9cff-8dcb9f196529",
   "metadata": {},
   "source": [
    "## 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
    "---\n",
    "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。\n",
    "\n",
    "例えば以下のようなx, w, bがあった場合は、\n",
    "```python\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）\n",
    "```\n",
    "出力は次のようになります。\n",
    "```python\n",
    "a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、特徴量数）\n",
    "```\n",
    "\n",
    "入力が2チャンネル、出力が3チャンネルの例です。計算グラフを書いた上で、バックプロパゲーションも手計算で考えてみましょう。計算グラフの中には和と積しか登場しないので、微分を新たに考える必要はありません。\n",
    "\n",
    "《補足》\n",
    "\n",
    "チャンネル数を加える場合、配列をどういう順番にするかという問題があります。(バッチサイズ、チャンネル数、特徴量数)または(バッチサイズ、特徴量数、チャンネル数)が一般的で、ライブラリによって順番は異なっています。（切り替えて使用できるものもあります）\n",
    "\n",
    "今回のスクラッチでは自身の実装上どちらが効率的かを考えて選んでください。上記の例ではバッチサイズは考えておらず、(チャンネル数、特徴量数)です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db5b1547-ec35-4c51-bacf-6475ef700255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    1D畳み込み層\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : ndarray\n",
    "      重み\n",
    "    B : ndarray\n",
    "      バイアス\n",
    "    \"\"\"\n",
    "    def __init__(self, W, B):\n",
    "        self.stride = 1     #今回は1固定\n",
    "        self.padding = 0    #今回は無し\n",
    "        self.batch_size = 1 # #今回は1固定\n",
    "        self.W = W\n",
    "        self.B = B\n",
    "\n",
    "        # 微分した重みとバイアス\n",
    "        self.dW = None\n",
    "        self.dB = None\n",
    "\n",
    "\n",
    "        self.x = None\n",
    "        self.x_matrix = None\n",
    "        self.dZ = None\n",
    "\n",
    "        # AdaGrad用のインスタンス変数\n",
    "        self.W_h = 0\n",
    "        self.B_h = 0\n",
    "\n",
    "    def _forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "\n",
    "        self.x = X  # backwardで使うため保持\n",
    "\n",
    "        idx = 0\n",
    "        n_output = self._get_output_size()\n",
    "        n_channel = self.W.shape[0]\n",
    "        n_filters = self.W.shape[2]\n",
    "        A = np.zeros((n_channel, n_output))\n",
    "        x_list = [] # backwardで使用する\n",
    "\n",
    "        # 畳み込み演算用にXデータをフィルターと同じ形状に分割し、backwardで使用するため保持もする\n",
    "        for _ in range(n_output):\n",
    "            _X = X[:, idx:idx + n_filters]\n",
    "            x_list.append(_X) #1次元配列に形状を変更（平滑化）\n",
    "            # x_list.append(_X.reshape(1, -1)) #1次元配列に形状を変更（平滑化）\n",
    "            idx += self.stride\n",
    "\n",
    "        # backwardで使用するため保持する。\n",
    "        self.x_matrix = np.array(x_list).reshape(n_output, -1)\n",
    "\n",
    "        # 畳み込み演算\n",
    "        for i in range(n_channel):\n",
    "            for j, _X in enumerate(x_list):\n",
    "                A[i][j] = (np.dot(_X.reshape(1, -1), self.W[i].reshape(1, -1).T)) + self.B[i]\n",
    "\n",
    "        return A\n",
    "\n",
    "    def _backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dX : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "\n",
    "        n_output = self._get_output_size()\n",
    "        n_channel, n_features = self.x.shape\n",
    "        n_filters = self.W.shape[-1]\n",
    "        idx = diff = n_features - n_filters\n",
    "\n",
    "        w_list = []\n",
    "        for i in range(n_filters):\n",
    "            for _ in range(n_output):\n",
    "                _w = np.pad(self.W[i], [(0, 0), (idx-diff, diff)],'constant')\n",
    "                w_list.append(_w.reshape(1, -1))\n",
    "                diff -= self.stride\n",
    "            idx = diff = n_features - n_filters\n",
    "\n",
    "        w_matrix = np.array(w_list).reshape(len(w_list), -1)\n",
    "        dX = np.dot(dA.reshape(1, -1), w_matrix).reshape(self.x.shape)\n",
    "\n",
    "        self.dW = np.dot(dA, self.x_matrix)\n",
    "        self.dW = self.dW.reshape(self.W.shape)\n",
    "        self.dB = np.array(dA.sum(axis=0))\n",
    "\n",
    "        # # 更新\n",
    "        # self = self.optimizer.update(self)\n",
    "\n",
    "        return dX\n",
    "\n",
    "\n",
    "    def _get_output_size(self):\n",
    "        '''\n",
    "        出力サイズの計算 n_output = (n_features + 2*padding - n_filters)/stride + 1\n",
    "        '''\n",
    "\n",
    "        n_features = self.x.shape[1]\n",
    "        n_filters = self.W.shape[-1]\n",
    "        return int((n_features + 2*self.padding - n_filters) / self.stride + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3acc8751-971a-4d82-9ffd-40ef786ea850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA:\n",
      " [[16. 22.]\n",
      " [17. 23.]\n",
      " [18. 24.]]\n",
      "b:\n",
      " [[ 51. 120. 120.  69.]\n",
      " [ 51. 120. 120.  69.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）\n",
    "\n",
    "dA = np.array([16, 22, 17, 23, 18, 24]).reshape(3, 2)\n",
    "cnn = Conv1d(w, b)\n",
    "a = cnn._forward(x)\n",
    "print(\"dA:\\n\", a)\n",
    "b = cnn._backward(dA)\n",
    "print(\"b:\\n\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a73d61d-32ae-4cb7-8e9e-72f6eb9908bb",
   "metadata": {},
   "source": [
    "## 【問題8】学習と推定\n",
    "---\n",
    "これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えてMNISTを学習・推定し、Accuracyを計算してください。\n",
    "\n",
    "出力層だけは全結合層をそのまま使ってください。ただし、チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、 平滑化 を行なってください。\n",
    "\n",
    "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2dfbe064-3bc6-4f21-90e0-d6636d9c5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self, *shape):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        w = self.sigma * np.random.randn(*shape)\n",
    "        return w\n",
    "\n",
    "    def B(self, *shape):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        b = self.sigma * np.random.randn(*shape)\n",
    "        return b\n",
    "\n",
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavierによる初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        _ = sigma\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        sigma = 1.0 / np.sqrt(n_nodes1)\n",
    "        w = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return w\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        b = np.random.randn(n_nodes2)\n",
    "        return b\n",
    "\n",
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Heによる初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        _ = sigma\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        sigma = np.sqrt( 2.0 / n_nodes1)\n",
    "        w = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return w\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        b = np.random.randn(n_nodes2)\n",
    "        return b\n",
    "\n",
    "# 最適化手法\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "\n",
    "        return layer\n",
    "\n",
    "class AdaGrad:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        DELTA = 1e-7\n",
    "        layer.W_h += layer.dW * layer.dW\n",
    "        layer.W -= self.lr * (1.0 / np.sqrt(layer.W_h + DELTA)) * layer.dW / layer.x.shape[0]\n",
    "\n",
    "        layer.B_h += layer.dB * layer.dB\n",
    "        layer.B -= self.lr * (1.0 / np.sqrt(layer.B_h + DELTA)) * layer.dB / layer.x.shape[0]\n",
    "\n",
    "        return layer\n",
    "\n",
    "def cross_entropy_error(y, Z3):\n",
    "    '''\n",
    "    交差エントロピー誤差の計算\n",
    "    '''\n",
    "\n",
    "    DELTA = 1e-7\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(y * np.log(Z3 + DELTA))/batch_size\n",
    "\n",
    "# 活性化関数クラス化\n",
    "class Softmax:\n",
    "    \"\"\"\n",
    "    ソフトマックス関数\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        X = X.T\n",
    "        y = np.exp(X) / np.sum(np.exp(X), axis=0)\n",
    "\n",
    "        return y.T\n",
    "\n",
    "    def backward(self, Z3, y):\n",
    "        batch_size = y.shape[0]\n",
    "        ret = (Z3 - y)/batch_size\n",
    "\n",
    "        # lossの計算\n",
    "        self.loss = cross_entropy_error(y, Z3)\n",
    "\n",
    "        return ret\n",
    "\n",
    "class ReLU:\n",
    "    \"\"\"\n",
    "    ReLU関数\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        self.x = X\n",
    "\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def backward(self, X):\n",
    "\n",
    "        return np.where(self.x > 0, X, 0)\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    \"\"\"\n",
    "    ハイボリックタンジェント関数\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.dA = None\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        self.dA = np.tanh(X)\n",
    "        return self.dA\n",
    "\n",
    "    def backward(self, dZ):\n",
    "\n",
    "        return dZ*(1 - np.tanh(self.dA)**2)\n",
    "\n",
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.initializer = initializer\n",
    "\n",
    "        self.W = self.initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        self.B = self.initializer.B(self.n_nodes2)\n",
    "\n",
    "        # 微分した重みとバイアス\n",
    "        self.dW = None\n",
    "        self.dB = None\n",
    "\n",
    "        self.x = None\n",
    "\n",
    "        # AdaGrad用のインスタンス変数\n",
    "        self.W_h = 0\n",
    "        self.B_h = 0\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "\n",
    "        self.x = X  # backwardで使うため保持\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dA)\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "\n",
    "        return dZ\n",
    "\n",
    "\n",
    "\n",
    "class ScratchConv1d:\n",
    "    \"\"\"\n",
    "    畳み込み層\n",
    "    Parameters\n",
    "    ----------\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    n_filters : int\n",
    "      フィルター数\n",
    "    stride : int\n",
    "      ストライド\n",
    "    padding : int\n",
    "      パディング\n",
    "    in_channels : int\n",
    "      入力チャンネル数\n",
    "    n_out_channels : int\n",
    "      出力チャンネル数\n",
    "    \"\"\"\n",
    "    def __init__(self, initializer, optimizer, n_filters, stride=1, padding=1, in_channels=1, n_out_channels=1):\n",
    "        self.optimizer = optimizer\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.n_output = None\n",
    "        self.in_channels = in_channels\n",
    "        self.n_out_channels = n_out_channels\n",
    "        self.n_filters = n_filters\n",
    "\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.initializer = initializer\n",
    "\n",
    "        # 重みとバイアスは全ての特徴量に共有で使用するため、FCのように各ノードごとに保持する必要はない\n",
    "        self.W = self.initializer.W(n_out_channels, in_channels, n_filters)\n",
    "        self.B = self.initializer.B(n_out_channels)\n",
    "\n",
    "        # 微分した重みとバイアス\n",
    "        self.dW = None\n",
    "        self.dB = None\n",
    "\n",
    "        self.x = None\n",
    "        self.x_matrix = None\n",
    "        self.dZ = None\n",
    "\n",
    "        # AdaGrad用のインスタンス変数\n",
    "        self.W_h = 0\n",
    "        self.B_h = 0\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "\n",
    "        self.x = X  # backwardで使うため保持\n",
    "\n",
    "        idx = 0\n",
    "        n_filters = self.W.shape[2]\n",
    "        self.n_output = self.get_output_size(self.x.shape[1], n_filters)\n",
    "        n_channel = self.W.shape[0]\n",
    "        A = np.zeros((n_channel, self.n_output))\n",
    "        x_list = [] # backwardで使用する\n",
    "\n",
    "        # 畳み込み演算用にXデータをフィルターと同じ形状に分割し、backwardで使用するため保持もする\n",
    "        for _ in range(self.n_output):\n",
    "            _X = X[:, idx:idx + n_filters]\n",
    "            x_list.append(_X) #1次元配列に形状を変更（平滑化）\n",
    "            idx += self.stride\n",
    "\n",
    "        # backwardで使用するため保持する。\n",
    "        self.x_matrix = np.array(x_list).reshape(self.n_output, -1)\n",
    "\n",
    "        # 畳み込み演算\n",
    "        for i in range(n_channel):\n",
    "            for j, _X in enumerate(x_list):\n",
    "                A[i][j] = (np.dot(_X.reshape(1, -1), self.W[i].reshape(1, -1).T)) + self.B[i]\n",
    "\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "\n",
    "        # n_row = len(self.x) - len(self.W) + self.stride\n",
    "        n_features = self.x.shape[1]\n",
    "        n_filters = self.W.shape[-1]\n",
    "        #TODO self.n_outputに置き換えれるかも\n",
    "        n_output = self.get_output_size(n_features, n_filters)\n",
    "        n_channel, n_features = self.x.shape\n",
    "        idx = diff = n_features - n_filters\n",
    "\n",
    "\n",
    "        # 重みを平滑化にする\n",
    "        w_flatten = self.W.reshape(-1, self.n_filters)\n",
    "\n",
    "        w_list = []\n",
    "        dX = None\n",
    "        # for i in range(n_filters):\n",
    "        for i in range(self.n_out_channels):\n",
    "            for _ in range(n_output):\n",
    "                if self.in_channels == 1:\n",
    "                    _w = np.pad(w_flatten[i], [idx-diff, diff],'constant')\n",
    "                    w_list.append(_w)\n",
    "                else:\n",
    "                    _w = np.pad(self.W[i], [(0, 0), (idx-diff, diff)],'constant')\n",
    "                    w_list.append(_w.reshape(1, -1))\n",
    "                diff -= self.stride\n",
    "            idx = diff = n_features - n_filters\n",
    "\n",
    "        if self.in_channels == 1:\n",
    "            dX = np.sum(dA.reshape(-1,1) * np.array(w_list), axis=0)\n",
    "            dX = dX.reshape(1, -1)\n",
    "            self.dW = np.sum(self.x_matrix.T * dA, axis=1)\n",
    "            self.dB = np.array(dA.sum())\n",
    "\n",
    "        else:\n",
    "            w_matrix = np.array(w_list).reshape(len(w_list), -1)\n",
    "            dX = np.dot(dA.reshape(1, -1), w_matrix).reshape(self.x.shape)\n",
    "            self.dW = np.dot(dA, self.x_matrix)\n",
    "            self.dW = self.dW.reshape(self.W.shape)\n",
    "            self.dB = np.array(dA.sum(axis=0))\n",
    "\n",
    "        # 更新\n",
    "        self.optimizer.update(self)\n",
    "\n",
    "        return dX\n",
    "\n",
    "    def get_output_size(self, n_features, n_filters):\n",
    "        '''\n",
    "        出力サイズの計算 n_output = (n_features + 2*padding - n_filters)/stride + 1\n",
    "        '''\n",
    "        return int((n_features + 2*self.padding - n_filters) / self.stride + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf6ef5-f708-4aa6-b50b-3368a83e5d61",
   "metadata": {},
   "source": [
    "### CNNのクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8a38250-b6df-42bc-9e40-9079e0251d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scratch1dCNNClassifier:\n",
    "    def __init__(self, batch_size, n_features, n_nodes1, n_nodes2, n_output, lr, sigma=0.01, optimizer=AdaGrad, activation=Tanh, itializer=XavierInitializer):\n",
    "        self.batch_size = batch_size\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes1 = n_nodes1    # 1層目のノード数\n",
    "        self.n_nodes2 = n_nodes2    # 2層目のノード数\n",
    "        self.n_output = n_output    # 出力層のノード数\n",
    "        self.lr = lr        # 学習率\n",
    "        self.sigma = sigma  # ガウス分布の標準偏差\n",
    "\n",
    "        self.itializer = itializer\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        self.Conv1d = ScratchConv1d(initializer=SimpleInitializer(0.01), optimizer=self.optimizer(self.lr), n_filters=8, stride=1, padding=0, in_channels=1, n_out_channels=1)\n",
    "        self.Conv1d.n_output = self.Conv1d.get_output_size(n_features, self.Conv1d.W.shape[2])\n",
    "        self.activation1 = self.activation()\n",
    "        self.FC2 = FC(self.Conv1d.n_output, self.n_nodes2, self.itializer(self.sigma), self.optimizer(self.lr))\n",
    "        self.activation2 = self.activation()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, self.itializer(self.sigma), self.optimizer(self.lr))\n",
    "        self.activation3 = Softmax()\n",
    "\n",
    "        self.loss_list = []\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "\n",
    "        Z3 = self._forward(X)\n",
    "        self._backward(X, y, Z3)\n",
    "\n",
    "        # 損失関数の値を保持\n",
    "        self.loss_list.append(self.activation3.loss)\n",
    "\n",
    "\n",
    "    def _forward(self, X):\n",
    "\n",
    "        A1 = self.Conv1d.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "\n",
    "        return Z3\n",
    "\n",
    "\n",
    "    def _backward(self, X, y, Z3):\n",
    "\n",
    "        dA3 = self.activation3.backward(Z3, y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "        dZ2 = self.FC3.backward(dA3)\n",
    "        dA2 = self.activation2.backward(dZ2)\n",
    "        dZ1 = self.FC2.backward(dA2)\n",
    "        dA1 = self.activation1.backward(dZ1)\n",
    "        dZ0 = self.Conv1d.backward(dA1) # dZ0は使用しない\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_features)\n",
    "            特徴量ベクトル\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred：次の形のndarray, shape (batch_size, n_output)\n",
    "            推定結果（10個の確率の中で、最も高いインデックス＝各ラベル（0〜9））\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred_list = []\n",
    "        for _, _x in enumerate(X):\n",
    "            y_pred = self._forward(_x.reshape(1, -1))\n",
    "            y_pred_list.append(y_pred.argmax(axis=1))\n",
    "\n",
    "        return np.array(y_pred_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ca357-cb92-4268-be3d-87f26a712be8",
   "metadata": {},
   "source": [
    "### ミニバッチ関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "656b8187-9fe7-4a64-b653-c312434bffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "\n",
    "def get_mini_batch_data():\n",
    "    from keras.datasets import mnist\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    X_train = X_train.reshape(-1, 784)\n",
    "    X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "    X_train = X_train.astype(np.float)\n",
    "    X_test = X_test.astype(np.float)\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "    # One-hotエンコーダー\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "    y_val_one_hot = enc.transform(y_val[:, np.newaxis])\n",
    "\n",
    "    print(y_train_one_hot.shape)\n",
    "    print(y_val_one_hot.shape)\n",
    "\n",
    "    data = GetMiniBatch(X_train, y_train_one_hot, batch_size=20)\n",
    "\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71363f-b3c2-4754-8add-7f3e9e28e99d",
   "metadata": {},
   "source": [
    "### データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07252937-91a8-4610-9f70-d86c7dde5394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(12000, 784)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# print(\"X:\", X_train)\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# One-hotエンコーダー\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d765210-e60d-4811-8153-c90434e2a746",
   "metadata": {},
   "source": [
    "### 学習と推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0bda648-bd2b-4d67-b786-f4d6372ae8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1 # バッチサイズ\n",
    "n_features = 784 # 特徴量の数\n",
    "n_nodes1 = 400 # 1層目のノード数\n",
    "n_nodes2 = 200 # 2層目のノード数\n",
    "n_output = 10 # 出力のクラス数（3層目のノード数）\n",
    "lr = 0.01 # 学習率\n",
    "epoch = 10 #エポック回数\n",
    "\n",
    "cnn = Scratch1dCNNClassifier(batch_size, n_features, n_nodes1, n_nodes2, n_output, lr)\n",
    "\n",
    "for p in range(epoch):\n",
    "\n",
    "    get_mini_batch = GetMiniBatch(X_train, y_train, batch_size=batch_size, seed=p)\n",
    "\n",
    "    for i in range(3000):\n",
    "\n",
    "        mini_X_train, mini_y_train = get_mini_batch[i]\n",
    "        # mini_X_val, mini_y_val = val_get_mini_batch[i]\n",
    "        cnn.fit(mini_X_train, mini_y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0fdcb4bd-20fc-4539-8d04-40f675317528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score : 0.925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = cnn.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(\"score :\", score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
