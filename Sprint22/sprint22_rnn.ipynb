{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e0c22f8-0c4d-4a26-af8e-2edae4650984",
   "metadata": {},
   "source": [
    "# Sprint22 リカレントニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb58d12f-01d1-4129-a5d2-15b6a1906921",
   "metadata": {},
   "source": [
    "## 【問題1】SimpleRNNのフォワードプロパゲーション実装\n",
    "---\n",
    "SimpleRNNのクラスSimpleRNNを作成してください。基本構造はFCクラスと同じになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334f1c9c-4d7e-489f-930f-e0939de16d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleInitializerRNN:\n",
    "    \"\"\"\n",
    "    RNN用の初期化\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        pass\n",
    "    def Wx(self, n_features, n_nodes):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_features : int\n",
    "          特徴量の数\n",
    "        n_nodes : int\n",
    "          ノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        w = np.zeros((n_features, n_nodes))\n",
    "        return w\n",
    "\n",
    "    def Wh(self, n_nodes):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes : int\n",
    "          ノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        w = np.zeros((n_nodes, n_nodes))\n",
    "        return w\n",
    "\n",
    "    def B(self, n_nodes):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes : int\n",
    "          ノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        b = np.zeros(n_nodes)\n",
    "        return b\n",
    "\n",
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavierによる初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        _ = sigma\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        sigma = 1.0 / np.sqrt(n_nodes1)\n",
    "        w = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return w\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        b = np.random.randn(n_nodes2)\n",
    "        return b\n",
    "\n",
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Heによる初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        _ = sigma\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        sigma = np.sqrt( 2.0 / n_nodes1)\n",
    "        w = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return w\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        b = np.random.randn(n_nodes2)\n",
    "        return b\n",
    "\n",
    "# 活性化関数クラス化\n",
    "class Softmax:\n",
    "    \"\"\"\n",
    "    ソフトマックス関数\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        X = X.T\n",
    "        y = np.exp(X) / np.sum(np.exp(X), axis=0)\n",
    "\n",
    "        return y.T\n",
    "\n",
    "    def backward(self, Z3, y):\n",
    "        batch_size = y.shape[0]\n",
    "        ret = (Z3 - y)/batch_size\n",
    "\n",
    "        # lossの計算\n",
    "        self.loss = cross_entropy_error(y, Z3)\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    \"\"\"\n",
    "    ReLU関数\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        self.x = X\n",
    "\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def backward(self, X):\n",
    "\n",
    "        return np.where(self.x > 0, X, 0)\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    \"\"\"\n",
    "    ハイボリックタンジェント関数\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.dA = None\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        self.dA = np.tanh(X)\n",
    "        return self.dA\n",
    "\n",
    "    def backward(self, dZ):\n",
    "\n",
    "        return dZ*(1 - np.tanh(self.dA)**2)\n",
    "\n",
    "\n",
    "# 最適化手法\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, n_features, n_nodes, optimizer, initializer):\n",
    "        self.optimizer = optimizer\n",
    "        self.initializer = initializer\n",
    "\n",
    "        self.Wx = self.initializer.Wx(n_features, n_nodes)\n",
    "        self.Wh = self.initializer.Wh(n_nodes)\n",
    "        self.B = self.initializer.B(n_nodes)\n",
    "\n",
    "        self.h = None\n",
    "        self.x = None\n",
    "\n",
    "        # 微分した重みとバイアス\n",
    "        self.dWx = None\n",
    "        self.dWh = None\n",
    "        self.dB = None\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx = self.Wx\n",
    "        Wh = self.Wh\n",
    "        B = self.B\n",
    "        batch_size, n_sequences, n_features = xs.shape\n",
    "        n_features, n_nodes = Wx.shape\n",
    "\n",
    "        hs = np.empty((batch_size, n_sequences, n_nodes), dtype='f')\n",
    "\n",
    "        for seq in range(n_sequences):\n",
    "\n",
    "            x = xs[:, seq, :]\n",
    "            h_prev = self.h\n",
    "\n",
    "            # ここでRNNの計算\n",
    "            t = np.dot(h_prev, Wh) + np.dot(x, Wx) + B\n",
    "            h_next = np.tanh(t)\n",
    "\n",
    "            # backwardで使用するため保持\n",
    "            self.x = x\n",
    "            self.h = h_next\n",
    "            hs[:, seq, :] = self.h\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dA):\n",
    "        pass\n",
    "\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "class ScratchSimpleRNNClassifier:\n",
    "    def __init__(self, batch_size, n_features, n_nodes, lr, sigma=0.01, optimizer=SGD, activation=Tanh, initializer=SimpleInitializerRNN):\n",
    "        self.batch_size = batch_size\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes = n_nodes\n",
    "        self.lr = lr        # 学習率\n",
    "        self.sigma = sigma  # ガウス分布の標準偏差\n",
    "\n",
    "        # self.optimizer = optimizer(self.lr)\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        self.RNN1 = SimpleRNN(self.n_features, self.n_nodes, self.initializer(self.sigma), self.optimizer(self.lr))\n",
    "        self.activation1 = self.activation()\n",
    "        self.RNN2 = SimpleRNN(self.n_features, self.n_nodes, self.initializer(self.sigma), self.optimizer(self.lr))\n",
    "        self.activation2 = self.activation()\n",
    "        self.RNN3 = SimpleRNN(self.n_features, self.n_nodes, self.initializer(self.sigma), self.optimizer(self.lr))\n",
    "        self.activation3 = Softmax()\n",
    "\n",
    "        self.loss_list = []\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "\n",
    "        Z3 = self._forward(X)\n",
    "        self._backward(X, y, Z3)\n",
    "\n",
    "        # 損失関数の値を保持\n",
    "        self.loss_list.append(self.activation3.loss)\n",
    "\n",
    "\n",
    "    def _forward(self, X):\n",
    "\n",
    "        A1 = self.RNN1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.RNN2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        A3 = self.RNN3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "\n",
    "        return Z3\n",
    "\n",
    "\n",
    "    def _backward(self, X, y, Z3):\n",
    "\n",
    "        dA3 = self.activation3.backward(Z3, y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "        dZ2 = self.RNN3.backward(dA3)\n",
    "        dA2 = self.activation2.backward(dZ2)\n",
    "        dZ1 = self.RNN2.backward(dA2)\n",
    "        dA1 = self.activation1.backward(dZ1)\n",
    "        dZ0 = self.RNN1.backward(dA1) # dZ0は使用しない\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_features)\n",
    "            特徴量ベクトル\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred：次の形のndarray, shape (batch_size, n_output)\n",
    "            推定結果（10個の確率の中で、最も高いインデックス＝各ラベル（0〜9））\n",
    "        \"\"\"\n",
    "        y_pred = self._forward(X)\n",
    "\n",
    "        return y_pred.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80378c31-2785-442d-84b8-8e45f63daab1",
   "metadata": {},
   "source": [
    "## 【問題2】小さな配列でのフォワードプロパゲーションの実験\n",
    "---\n",
    "小さな配列でフォワードプロパゲーションを考えてみます。\n",
    "\n",
    "入力x、初期状態h、重みw_xとw_h、バイアスbを次のようにします。\n",
    "\n",
    "ここで配列xの軸はバッチサイズ、系列数、特徴量数の順番です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb73385-741e-432e-a472-d661903bc9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "b = np.array([1, 1, 1, 1]) # (n_nodes,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f639c7d5-36a0-4b4b-aaf8-63bc5bdb8bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.76188797 0.76213956 0.762391   0.7625584 ]\n",
      "  [0.792209   0.8141834  0.8340491  0.84977716]\n",
      "  [0.79494226 0.81839    0.8393965  0.85584176]]]\n",
      "[[0.79494228 0.81839002 0.83939649 0.85584174]]\n"
     ]
    }
   ],
   "source": [
    "rnn = SimpleRNN(n_features, n_nodes, SGD, SimpleInitializerRNN(0.05))\n",
    "rnn.Wh = w_h\n",
    "rnn.Wx = w_x\n",
    "rnn.B = b\n",
    "rnn.h = h\n",
    "hs = rnn.forward(x)\n",
    "print(hs)\n",
    "print(rnn.h)\n",
    "# h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]]) # (batch_size, n_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e50f59-29b8-46ce-9f8c-dbcef9c1146a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
